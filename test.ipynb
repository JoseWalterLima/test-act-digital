{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bibliotecas necessárias e configurações de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, desc, udf, broadcast\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Set variáveis de ambiente\n",
    "os.environ['PYSPARK_PYTHON']=sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=sys.executable\n",
    "\n",
    "# Cria uma sessão Spark\n",
    "spark=SparkSession.builder \\\n",
    "    .appName(\"actdigital\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados disponibilizados\n",
    "data=[\n",
    "    (\"Alice\", 34, \"Data Scientist\"),\n",
    "    (\"Bob\", 45, \"Data Engineer\"),\n",
    "    (\"Cathy\", 29, \"Data Analyst\"),\n",
    "    (\"David\", 35, \"Data Scientist\")\n",
    "]\n",
    "columns=[\"Name\", \"Age\", \"Occupation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 1: Manipulação de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crie um DataFrame a partir dos dados fornecidos\n",
    "df=spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|Cathy| 29|\n",
      "|David| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecione apenas as colunas \"Name\" e \"Age\" \n",
    "df.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtre as linhas onde a \"Age\" é maior que 30\n",
    "df.filter(\"Age > 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|    Occupation|avg(Age)|\n",
      "+--------------+--------+\n",
      "|Data Scientist|    34.5|\n",
      "| Data Engineer|    45.0|\n",
      "|  Data Analyst|    29.0|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupe os dados pelo campo \"Occupation\" e calcule a média de \"Age\" para cada grupo\n",
    "df.groupBy(\"Occupation\").agg(\n",
    "    avg(\"Age\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|    Occupation|avg(Age)|\n",
      "+--------------+--------+\n",
      "| Data Engineer|    45.0|\n",
      "|Data Scientist|    34.5|\n",
      "|  Data Analyst|    29.0|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordene o DataFrame resultante da questão anterior pela média de \"Age\" em ordem decrescente\n",
    "df.groupBy(\"Occupation\").agg(\n",
    "    avg(\"Age\")).orderBy(desc(\n",
    "    \"avg(Age)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 2: Funções Avançadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+-----------+\n",
      "| Name|Age|    Occupation|AgeCategory|\n",
      "+-----+---+--------------+-----------+\n",
      "|Alice| 34|Data Scientist|     Adulto|\n",
      "|  Bob| 45| Data Engineer|     Senior|\n",
      "|Cathy| 29|  Data Analyst|      Jovem|\n",
      "|David| 35|Data Scientist|     Adulto|\n",
      "+-----+---+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crie uma função em Python que converte idades para categorias\n",
    "# e aplique esta função ao DataFrame usando uma UDF\n",
    "\n",
    "# Função python\n",
    "def age_category(age):\n",
    "    if age < 30:\n",
    "        return \"Jovem\"\n",
    "    elif 30 < age < 40:\n",
    "        return \"Adulto\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "# Converter para UDF\n",
    "age_category_udf=udf(age_category, StringType())\n",
    "\n",
    "# Criar coluna com categorias\n",
    "df=df.withColumn(\"AgeCategory\", age_category_udf(df[\"Age\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+-----------+----------------+-------------+\n",
      "| Name|Age|    Occupation|AgeCategory|AvgOccupationAge|AgeDifference|\n",
      "+-----+---+--------------+-----------+----------------+-------------+\n",
      "|Cathy| 29|  Data Analyst|      Jovem|            29.0|          0.0|\n",
      "|  Bob| 45| Data Engineer|     Senior|            45.0|          0.0|\n",
      "|Alice| 34|Data Scientist|     Adulto|            34.5|         -0.5|\n",
      "|David| 35|Data Scientist|     Adulto|            34.5|          0.5|\n",
      "+-----+---+--------------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adicione uma coluna que mostre a diferença de idade entre\n",
    "# cada indivíduo e a média de idade do seu \"Occupation\"\n",
    "\n",
    "# Especificar condicional da janela\n",
    "windowSpec=Window.partitionBy(\"Occupation\")\n",
    "\n",
    "# Calcular a idade média da ocupação\n",
    "df=df.withColumn(\"AvgOccupationAge\", avg(\n",
    "    \"Age\").over(windowSpec))\n",
    "\n",
    "# Diferença em relação à média da ocupação\n",
    "df=df.withColumn(\"AgeDifference\", col(\n",
    "    \"Age\") - col(\"AvgOccupationAge\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 3: Performance e Otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explique como o particionamento pode ser usado para melhorar a performance em operações de leitura e escrita de dados em PySpark.\n",
    "- No PySpark, o particionamento de dados refere-se ao processo de divisão de um grande conjunto de dados em pedaços ou partições menores, que podem ser processados ​​simultaneamente. Este é um aspecto importante da computação distribuída, pois permite que grandes conjuntos de dados sejam processados ​​de forma mais eficiente, dividindo a carga de trabalho entre várias máquinas ou processadores. Cada partição pode ser processada em paralelo por diferentes executores no cluster, permitindo que grandes conjuntos de dados sejam manipulados de forma mais eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dê um exemplo de código que particiona um DataFrame por uma coluna específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de particionamento na escrita de dados\n",
    "# utilizando como critério de partição a ocupação\n",
    "df.write.partitionBy(\"Occupation\").format(\"parquet\").save(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descreva o conceito de Broadcast Join em PySpark e como ele pode ser usado para otimizar operações de join.\n",
    "- O Broadcast Join é uma técnica de otimização em PySpark que melhora o desempenho das operações de junção (joins) quando uma das tabelas envolvidas é pequena o suficiente para caber na memória do executador. Em vez de realizar a junção de forma distribuída, que pode ser ineficiente e lenta, o Broadcast Join permite que a tabela pequena seja replicada e distribuída para todos os nós do cluster. Isso reduz a necessidade de shuffling e comunicação entre os nós durante o processo de junção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente um exemplo de Broadcast Join entre dois DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----------+\n",
      "| id|name|age|department|\n",
      "+---+----+---+----------+\n",
      "|  1|John| 30|        IT|\n",
      "|  2|Jane| 25|        HR|\n",
      "|  3|Mike| 35|   Finance|\n",
      "+---+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar uma sessão Spark\n",
    "spark=SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
    "\n",
    "# Criar DataFrames de exemplo\n",
    "data1=[\n",
    "    (1, \"John\", 30),\n",
    "    (2, \"Jane\", 25),\n",
    "    (3, \"Mike\", 35),\n",
    "    (4, \"Lisa\", 28)\n",
    "]\n",
    "columns1=[\"id\", \"name\", \"age\"]\n",
    "df1=spark.createDataFrame(data1, columns1)\n",
    "\n",
    "data2=[\n",
    "    (1, \"IT\"),\n",
    "    (2, \"HR\"),\n",
    "    (3, \"Finance\")\n",
    "]\n",
    "columns2=[\"id\", \"department\"]\n",
    "df2=spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Realizar o Broadcast Join\n",
    "result = df1.join(broadcast(df2), on=\"id\", how=\"inner\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 4: Integração com Outras Tecnologias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstre como ler dados de um arquivo CSV e escrever o resultado em um formato Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma sessão Spark\n",
    "spark=SparkSession.builder \\\n",
    "    .appName(\"CSVparaParquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "csv_file_path='data/professions.csv'\n",
    "\n",
    "# Ler arquivo CSV em um DataFrame\n",
    "df=spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrever dados em parquet\n",
    "df.write.parquet('data/professions.parquet', mode='overwrite')\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explique como PySpark se integra com o Hadoop HDFS para leitura e escrita de dados.\n",
    "- O PySpark se integra com o Hadoop HDFS (Hadoop Distributed File System) para leitura e escrita de dados, permitindo que o Spark aproveite a escalabilidade e a resiliência do HDFS. A integração entre PySpark e HDFS é uma característica fundamental para trabalhar com grandes volumes de dados em ambientes distribuídos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dê um exemplo de código que leia um arquivo do HDFS e salve o resultado de volta no HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma sessão Spark\n",
    "spark=SparkSession.builder \\\n",
    "    .appName(\"ReadAndWriteToHDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do arquivo CSV no HDFS\n",
    "arquivo=\"hdfs://namenode:9000/path/to/input.csv\"\n",
    "\n",
    "# Caminho para salvar o arquivo Parquet no HDFS\n",
    "pasta_salvar=\"hdfs://namenode:9000/path/to/output.parquet\"\n",
    "\n",
    "# Ler dados do arquivo CSV no HDFS\n",
    "df=spark.read.csv(arquivo, header=True, inferSchema=True)\n",
    "\n",
    "# Realizar alguma transformação, como por exemplo,\n",
    "# adicionar uma nova coluna com valor constante\n",
    "df=df.withColumn(\"nova_coluna\", df[\"coluna_antiga\"] + 10)\n",
    "\n",
    "# Escrever o DataFrame transformado de volta para o HDFS no formato Parquet\n",
    "df.write.parquet(pasta_salvar, mode='overwrite')\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 5: Problema de Caso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere que você tem um grande arquivo de log com as seguintes colunas: \"timestamp\", \"user_id\", \"action\". Cada linha representa uma ação realizada por um usuário em um determinado momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregue o arquivo de log em um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+\n",
      "|          timestamp|user_id|  action|\n",
      "+-------------------+-------+--------+\n",
      "|2023-04-13 18:22:23|     41|purchase|\n",
      "|2023-05-12 06:46:37|     27|purchase|\n",
      "|2023-01-07 11:30:50|     37|purchase|\n",
      "|2023-04-27 00:31:34|     12|    view|\n",
      "|2023-12-01 05:14:59|     75|purchase|\n",
      "|2023-12-28 11:11:23|     18|  logout|\n",
      "|2023-01-27 13:19:35|     29|   login|\n",
      "|2023-02-22 19:01:09|     35|  logout|\n",
      "|2023-07-21 04:08:05|     75|purchase|\n",
      "|2023-08-20 23:59:29|     61|   click|\n",
      "|2023-07-05 19:23:02|     88|   login|\n",
      "|2023-09-11 02:49:11|     46|  logout|\n",
      "|2023-02-12 11:02:10|     48|  logout|\n",
      "|2023-06-24 22:14:24|     64|   click|\n",
      "|2023-02-08 16:34:45|      4|purchase|\n",
      "|2023-09-21 15:31:24|     16|   login|\n",
      "|2023-03-05 02:07:22|     44|    view|\n",
      "|2023-01-05 06:41:35|     79|purchase|\n",
      "|2023-11-02 19:44:41|      2|    view|\n",
      "|2023-02-08 09:20:24|     94|   login|\n",
      "+-------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicializa a SparkSession\n",
    "spark=SparkSession.builder \\\n",
    "    .appName(\"LogAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carrega o arquivo de log em um DataFrame\n",
    "file_path=\"data/log_data.csv\"\n",
    "log_df=spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "log_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conte o número de ações realizadas por cada usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|     31|   22|\n",
      "|     85|   28|\n",
      "|     65|   16|\n",
      "|     53|   32|\n",
      "|     78|   30|\n",
      "|     34|   19|\n",
      "|     81|   24|\n",
      "|     28|   16|\n",
      "|     76|   20|\n",
      "|     27|   20|\n",
      "|     26|   15|\n",
      "|     44|   20|\n",
      "|     12|   19|\n",
      "|     91|   25|\n",
      "|     22|   15|\n",
      "|     93|   21|\n",
      "|     47|   15|\n",
      "|      1|   21|\n",
      "|     52|   20|\n",
      "|     13|   31|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conta o número de ações realizadas por cada usuário\n",
    "user_actions_df=log_df.groupBy(\"user_id\").count()\n",
    "user_actions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontre os 10 usuários mais ativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|     53|   32|\n",
      "|     13|   31|\n",
      "|     78|   30|\n",
      "|     75|   30|\n",
      "|     61|   29|\n",
      "|      8|   29|\n",
      "|     50|   29|\n",
      "|     85|   28|\n",
      "|     68|   28|\n",
      "|     15|   27|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encontra os 10 usuários mais ativos\n",
    "top_users_df=user_actions_df.orderBy(col(\"count\").desc()).limit(10)\n",
    "top_users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salve o resultado em um arquivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o resultado em um arquivo CSV\n",
    "output_path = \"data/logs.csv\"\n",
    "top_users_df.write.csv(output_path, header=True, mode='overwrite')\n",
    "# Fecha a SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
